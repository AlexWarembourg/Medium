{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_id@store_id</th>\n",
       "      <th>d</th>\n",
       "      <th>y</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37838085</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>WI@WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37838086</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>WI@WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37838087</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>WI@WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37838088</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>WI@WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37838089</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>WI@WI_3</td>\n",
       "      <td>d_1941</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id      item_id  dept_id cat_id store_id  \\\n",
       "37838085  FOODS_3_823_WI_3_evaluation  FOODS_3_823  FOODS_3  FOODS     WI_3   \n",
       "37838086  FOODS_3_824_WI_3_evaluation  FOODS_3_824  FOODS_3  FOODS     WI_3   \n",
       "37838087  FOODS_3_825_WI_3_evaluation  FOODS_3_825  FOODS_3  FOODS     WI_3   \n",
       "37838088  FOODS_3_826_WI_3_evaluation  FOODS_3_826  FOODS_3  FOODS     WI_3   \n",
       "37838089  FOODS_3_827_WI_3_evaluation  FOODS_3_827  FOODS_3  FOODS     WI_3   \n",
       "\n",
       "         state_id state_id@store_id       d  y       date  ...  wday month  \\\n",
       "37838085       WI           WI@WI_3  d_1941  1 2016-05-22  ...     2     5   \n",
       "37838086       WI           WI@WI_3  d_1941  0 2016-05-22  ...     2     5   \n",
       "37838087       WI           WI@WI_3  d_1941  2 2016-05-22  ...     2     5   \n",
       "37838088       WI           WI@WI_3  d_1941  0 2016-05-22  ...     2     5   \n",
       "37838089       WI           WI@WI_3  d_1941  1 2016-05-22  ...     2     5   \n",
       "\n",
       "          year  event_name_1  event_type_1  event_name_2  event_type_2  \\\n",
       "37838085  2016            30             4             3             2   \n",
       "37838086  2016            30             4             3             2   \n",
       "37838087  2016            30             4             3             2   \n",
       "37838088  2016            30             4             3             2   \n",
       "37838089  2016            30             4             3             2   \n",
       "\n",
       "          snap_CA  snap_TX  snap_WI  \n",
       "37838085        0        0        0  \n",
       "37838086        0        0        0  \n",
       "37838087        0        0        0  \n",
       "37838088        0        0        0  \n",
       "37838089        0        0        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean, SeasonalRollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "import lightgbm as lgb \n",
    "from copy import copy\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, TopDown, MinTrace, ERM, OptimalCombination\n",
    "from IPython.display import display \n",
    "\n",
    "\n",
    "skip = [f\"d_{i}\" for i in range(1, 700 + 1)] \n",
    "HORIZON = 28 # horizon de forecast\n",
    "EXTRA_FEATURES = [\n",
    "           'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2',\n",
    "           'snap_CA', 'snap_TX', 'snap_WI'\n",
    "           ]\n",
    "\n",
    "# spec de la hierarchie que l'on veut travaillé \n",
    "spec = [\n",
    "    ['state_id', 'store_id', 'cat_id'], \n",
    "    ['state_id', 'store_id', 'cat_id', 'dept_id'],\n",
    "    ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id'],\n",
    "]\n",
    "\n",
    "calendar = pd.read_csv(\"/home/jupyter/mawa/data/calendar.csv\")\n",
    "row_sales = pd.read_csv(\"/home/jupyter/mawa/data/sales_train_evaluation.csv\")\n",
    "prices = pd.read_csv('/home/jupyter/mawa/data/sell_prices.csv')\n",
    "row_sales['state_id@store_id'] = row_sales['state_id'].astype(str) + \"@\" + row_sales['store_id'].astype(str)\n",
    "all_state_and_store = np.unique(row_sales['state_id@store_id'])\n",
    "\n",
    "# formattage ligne et renommage des colonnes pour respecter les conventions de la librairie.\n",
    "train = (row_sales\n",
    "         .drop(skip, axis=1)\n",
    "         .pipe(pd.melt, \n",
    "               id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \n",
    "                        \"store_id\", \"state_id\", \"state_id@store_id\"],\n",
    "               var_name='d',\n",
    "               value_name=\"y\"\n",
    "               )\n",
    "         .merge(calendar, how=\"inner\", on=[\"d\"])\n",
    "         .assign(date= lambda x : pd.to_datetime(x['date']))\n",
    "        )\n",
    "\n",
    "for col in EXTRA_FEATURES:\n",
    "    train[f\"{col}\"] = train[col].fillna('nan').astype(\"category\").cat.codes.astype(int)\n",
    "\n",
    "# extract all series id\n",
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('exog_data.pkl'):\n",
    "    X_df = (prices\n",
    "         #prices[prices['wm_yr_wk'] >= train['wm_yr_wk'].max()]\n",
    "        .merge(calendar[[\"wm_yr_wk\", \"date\"]], on='wm_yr_wk')\n",
    "        .merge(train[[\"id\", \"store_id\", \"item_id\", \"cat_id\", \"dept_id\", \"state_id\"]].drop_duplicates(),\n",
    "               on=['item_id', 'store_id'], how=\"left\")\n",
    "    )\n",
    "\n",
    "    X_all_df = [] \n",
    "    for ind_spec in spec:\n",
    "        pdf = (X_df\n",
    "               .groupby(ind_spec + [\"date\"])\n",
    "               .agg(sell_price = ('sell_price', 'mean'))\n",
    "               .reset_index()\n",
    "               .rename(columns={'date':'ds'})\n",
    "               .sort_values(by=\"ds\")\n",
    "               .assign(\n",
    "                   unique_id = lambda x : x[ind_spec].agg('/'.join, axis=1),\n",
    "                   momentum_price = lambda x : (x['sell_price'] / x.groupby(ind_spec)['sell_price']\n",
    "                                                .transform('shift')\n",
    "                                               ).fillna(0),\n",
    "               )\n",
    "               .set_index('unique_id')\n",
    "              )\n",
    "        X_all_df.append(pdf)\n",
    "\n",
    "    X_all_df_ = pd.concat(X_all_df).reset_index()[['unique_id', \"ds\", \"sell_price\", \"momentum_price\"]]\n",
    "    X_all_df_ = X_all_df_.merge(calendar.rename(columns={\"date\":\"ds\"})[[\"ds\", 'event_name_1', \n",
    "                                                        'event_name_2', 'event_type_1',\n",
    "                                                        'event_type_2', 'snap_CA', 'snap_TX', \n",
    "                                                        'snap_WI']], how=\"left\", on=[\"ds\"])\n",
    "    for col in EXTRA_FEATURES:\n",
    "        X_all_df_[col] = X_all_df_[col].fillna('nan').astype(\"category\").cat.codes.astype(int)\n",
    "        \n",
    "    X_all_df_.to_pickle('exog_data.pkl')\n",
    "    import gc\n",
    "    del X_all_df, pdf, X_df\n",
    "    gc.collect()\n",
    "else:\n",
    "    X_all_df_ = pd.read_pickle(\"exog_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up reconciliation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_fcst = ['LGBMRegressor', \n",
    "               'LGBMRegressor/BottomUp',\n",
    "               'LGBMRegressor/TopDown_method-forecast_proportions',\n",
    "               'LGBMRegressor/MinTrace_method-mint_shrink',\n",
    "               'LGBMRegressor/OptimalCombination_method-ols',\n",
    "                  ]\n",
    "    \n",
    "# Reconcile the base predictions\n",
    "reconcilers = [\n",
    "   BottomUp(),\n",
    "   TopDown(method='forecast_proportions'),\n",
    "   MinTrace(method='mint_shrink'), \n",
    "   OptimalCombination(method='ols'), \n",
    "   # ERM(method='reg'),\n",
    "  ]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "# paramètre basique pour le lightgbm\n",
    "# paramètre basique pour le lightgbm\n",
    "model_params = {\n",
    "    'verbose': -1,\n",
    "    'force_col_wise': True,\n",
    "    'num_leaves': 128,\n",
    "    'n_estimators': 500,\n",
    "}\n",
    "\n",
    "fcst = MLForecast(\n",
    "    models=[lgb.LGBMRegressor(**model_params)],\n",
    "    freq='D',\n",
    "    lags=[7 * (i+1) for i in range(8)],\n",
    "    lag_transforms = {\n",
    "        1:  [ExpandingMean()],\n",
    "        7:  [RollingMean(7), RollingMean(14), RollingMean(28), SeasonalRollingMean(7, 4)],\n",
    "        14: [RollingMean(7), RollingMean(14), RollingMean(28), SeasonalRollingMean(7, 4)],\n",
    "        28: [RollingMean(7), RollingMean(14), RollingMean(28), SeasonalRollingMean(7, 4)],\n",
    "    },\n",
    "    date_features=['month', 'day', 'dayofweek', 'quarter', 'week'],    \n",
    "    num_threads=70,\n",
    ")\n",
    "merge_calendar = (calendar\n",
    "                 .rename(columns={\"date\":\"ds\"})\n",
    "                 .assign(ds= lambda x : pd.to_datetime(x['ds']))\n",
    "                 )\n",
    "\n",
    "for col in EXTRA_FEATURES:\n",
    "    merge_calendar[col] = merge_calendar[col].fillna('nan').astype(\"category\").cat.codes\n",
    "\n",
    "def hierarchical_forecast(train, store, extra, spec, h, exog):\n",
    "    # transform the dataset to hierarchical format with the summing matrix (S_df) and a dict of the hierarchie.\n",
    "    Y_df, S_df, tags = aggregate(\n",
    "        train.loc[train['state_id@store_id'] == store].rename(columns={'date':'ds'}),\n",
    "        spec\n",
    "    )\n",
    "    # get series\n",
    "    all_series_at_bottom = tags['state_id/store_id/cat_id/dept_id/item_id']\n",
    "    \n",
    "    fcst_ = []\n",
    "    fitted = []\n",
    "    \n",
    "    # i dont want to fit all granularity in the same models.\n",
    "    # it allow to set the right features[series] for the right granularity \n",
    "    for series in spec:    \n",
    "        forecast_model = copy(fcst)\n",
    "        fitted_ds =  (Y_df\n",
    "                      .loc[tags['/'.join(series)]]\n",
    "                      .reset_index()\n",
    "                      .merge( (X_all_df_\n",
    "                               .assign(ds=lambda x : pd.to_datetime(x['ds']))\n",
    "                              ),\n",
    "                          how=\"left\", on=[\"ds\", \"unique_id\"]\n",
    "                      )\n",
    "                     )\n",
    "        fitted_ds = pd.concat(\n",
    "            (fitted_ds, \n",
    "            fitted_ds['unique_id'].str.split('/', expand=True).set_axis(series, axis=1)\n",
    "            ), axis=1)[['unique_id', 'ds', 'y'] + series + extra]\n",
    "        \n",
    "        for col in series:\n",
    "            fitted_ds[col] = fitted_ds[col].fillna('nan').astype(\"category\").cat.codes.astype(int)\n",
    "        \n",
    "        # fit model.\n",
    "        forecast_model.fit(\n",
    "            fitted_ds,\n",
    "            id_col='unique_id',\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            fitted=True,\n",
    "            static_features=series,\n",
    "            #max_horizon=h #one model per steps\n",
    "        )\n",
    "        # forecast 28 day ahead\n",
    "        Y_hat_df = forecast_model.predict(h, \n",
    "                                          X_df= (\n",
    "                                              X_all_df_.loc[X_all_df_['unique_id'].isin(Y_df.index.unique())]\n",
    "                                              .query(\"ds >= '2016-05-23'\")\n",
    "                                          )\n",
    "                                         )\n",
    "        fcst_.append(Y_hat_df)\n",
    "        # draw the fitted value \n",
    "        Y_fitted_df = forecast_model.forecast_fitted_values()\n",
    "        fitted.append(Y_fitted_df)\n",
    "        \n",
    "    # reconcile the all forecast.\n",
    "    Y_rec_df = hrec.reconcile(\n",
    "        Y_hat_df=pd.concat(fcst_), #Y_hat_df, \n",
    "        Y_df=pd.concat(fitted), #Y_fitted_df, \n",
    "        S=S_df, \n",
    "        tags=tags\n",
    "    )\n",
    "\n",
    "    bottom_level = (Y_rec_df\n",
    "                    .loc[all_series_at_bottom]\n",
    "                    .assign(blend_fcst=lambda x : x[bottom_fcst].mean(axis=1))\n",
    "                    .reset_index(names=\"unique_id\")\n",
    "                     .assign(unique_id= lambda x : (x['unique_id'].str.split('/', expand=True).iloc[:, -1]\n",
    "                                + '_' +\n",
    "                                x['unique_id'].str.split('/', expand=True).iloc[:, 1]) \n",
    "                                + '_evaluation'\n",
    "        )\n",
    "                   )\n",
    "    return bottom_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for CA@CA_1\n",
      "running for CA@CA_2\n",
      "running for CA@CA_3\n",
      "running for CA@CA_4\n",
      "running for TX@TX_1\n",
      "running for TX@TX_2\n",
      "running for TX@TX_3\n",
      "running for WI@WI_1\n",
      "running for WI@WI_2\n",
      "running for WI@WI_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>index</th>\n",
       "      <th>ds</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>index/BottomUp</th>\n",
       "      <th>LGBMRegressor/BottomUp</th>\n",
       "      <th>index/TopDown_method-forecast_proportions</th>\n",
       "      <th>LGBMRegressor/TopDown_method-forecast_proportions</th>\n",
       "      <th>index/MinTrace_method-mint_shrink</th>\n",
       "      <th>LGBMRegressor/MinTrace_method-mint_shrink</th>\n",
       "      <th>index/OptimalCombination_method-ols</th>\n",
       "      <th>LGBMRegressor/OptimalCombination_method-ols</th>\n",
       "      <th>blend_fcst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>0.959109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959109</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.836653</td>\n",
       "      <td>-3049.679773</td>\n",
       "      <td>0.873283</td>\n",
       "      <td>-3046.143386</td>\n",
       "      <td>0.636093</td>\n",
       "      <td>0.852850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>0.735005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735005</td>\n",
       "      <td>1.767324e-08</td>\n",
       "      <td>0.718589</td>\n",
       "      <td>-3049.673476</td>\n",
       "      <td>0.752092</td>\n",
       "      <td>-3046.139923</td>\n",
       "      <td>0.782667</td>\n",
       "      <td>0.744672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-25</td>\n",
       "      <td>0.693631</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.693631</td>\n",
       "      <td>1.366277e-07</td>\n",
       "      <td>0.665206</td>\n",
       "      <td>-3049.667179</td>\n",
       "      <td>0.671126</td>\n",
       "      <td>-3046.136460</td>\n",
       "      <td>0.637267</td>\n",
       "      <td>0.672172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-05-26</td>\n",
       "      <td>0.673162</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.673162</td>\n",
       "      <td>4.460956e-07</td>\n",
       "      <td>0.650177</td>\n",
       "      <td>-3049.660882</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>-3046.132996</td>\n",
       "      <td>0.614420</td>\n",
       "      <td>0.648258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-27</td>\n",
       "      <td>0.808946</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.808946</td>\n",
       "      <td>1.024028e-06</td>\n",
       "      <td>0.704992</td>\n",
       "      <td>-3049.654586</td>\n",
       "      <td>0.721610</td>\n",
       "      <td>-3046.129533</td>\n",
       "      <td>0.541779</td>\n",
       "      <td>0.717255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     unique_id  index         ds  LGBMRegressor  \\\n",
       "0  FOODS_1_001_CA_1_evaluation      0 2016-05-23       0.959109   \n",
       "1  FOODS_1_001_CA_1_evaluation      1 2016-05-24       0.735005   \n",
       "2  FOODS_1_001_CA_1_evaluation      2 2016-05-25       0.693631   \n",
       "3  FOODS_1_001_CA_1_evaluation      3 2016-05-26       0.673162   \n",
       "4  FOODS_1_001_CA_1_evaluation      4 2016-05-27       0.808946   \n",
       "\n",
       "   index/BottomUp  LGBMRegressor/BottomUp  \\\n",
       "0             0.0                0.959109   \n",
       "1             1.0                0.735005   \n",
       "2             2.0                0.693631   \n",
       "3             3.0                0.673162   \n",
       "4             4.0                0.808946   \n",
       "\n",
       "   index/TopDown_method-forecast_proportions  \\\n",
       "0                               0.000000e+00   \n",
       "1                               1.767324e-08   \n",
       "2                               1.366277e-07   \n",
       "3                               4.460956e-07   \n",
       "4                               1.024028e-06   \n",
       "\n",
       "   LGBMRegressor/TopDown_method-forecast_proportions  \\\n",
       "0                                           0.836653   \n",
       "1                                           0.718589   \n",
       "2                                           0.665206   \n",
       "3                                           0.650177   \n",
       "4                                           0.704992   \n",
       "\n",
       "   index/MinTrace_method-mint_shrink  \\\n",
       "0                       -3049.679773   \n",
       "1                       -3049.673476   \n",
       "2                       -3049.667179   \n",
       "3                       -3049.660882   \n",
       "4                       -3049.654586   \n",
       "\n",
       "   LGBMRegressor/MinTrace_method-mint_shrink  \\\n",
       "0                                   0.873283   \n",
       "1                                   0.752092   \n",
       "2                                   0.671126   \n",
       "3                                   0.630370   \n",
       "4                                   0.721610   \n",
       "\n",
       "   index/OptimalCombination_method-ols  \\\n",
       "0                         -3046.143386   \n",
       "1                         -3046.139923   \n",
       "2                         -3046.136460   \n",
       "3                         -3046.132996   \n",
       "4                         -3046.129533   \n",
       "\n",
       "   LGBMRegressor/OptimalCombination_method-ols  blend_fcst  \n",
       "0                                     0.636093    0.852850  \n",
       "1                                     0.782667    0.744672  \n",
       "2                                     0.637267    0.672172  \n",
       "3                                     0.614420    0.648258  \n",
       "4                                     0.541779    0.717255  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fcst = []\n",
    "for store in all_state_and_store:\n",
    "    print(f\"running for {store}\")\n",
    "    btm = hierarchical_forecast(\n",
    "        train, \n",
    "        store, \n",
    "        extra=EXTRA_FEATURES,\n",
    "        spec=spec,\n",
    "        h=HORIZON, \n",
    "        exog=X_all_df_\n",
    "    )\n",
    "    all_fcst.append(btm)\n",
    "    # btm.to_pickle(f'hier_{store}.pkl')\n",
    "pd.concat(all_fcst).to_pickle('all_fcst_hier.pkl')\n",
    "hier_fcst = pd.concat(all_fcst)\n",
    "hier_fcst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_frq = (train\n",
    " .loc[train['date'] >= train['date'].max() - pd.DateOffset(days=90)]\n",
    " .query(\"y == 0\")\n",
    " .groupby('id')\n",
    " .size()\n",
    " .to_frame('nb')\n",
    " .assign(freq = lambda x : x[\"nb\"] / 90)\n",
    " .reset_index()\n",
    ")\n",
    "\n",
    "fcst = pd.read_pickle('all_fcst_hier.pkl')\n",
    "fcst['yhat'] = fcst['LGBMRegressor/BottomUp'].clip(0, None)\n",
    "\n",
    "make_submission(fcst.merge(zero_frq.rename(columns={\"id\":\"unique_id\"}), how=\"left\", on=[\"unique_id\"]).assign(yhat=lambda x : np.where(\n",
    "    (x[\"LGBMRegressor/BottomUp\"].clip(0, None) > 0) & (x[\"freq\"] >= 0.95), 0, x[\"LGBMRegressor/BottomUp\"].clip(0, None))).rename(columns={\"unique_id\":\"id\", \"ds\":\"date\"}), \n",
    "                fcst_col=\"yhat\", filename=\"v3hier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_4130/2742455083.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation[\"id\"] = evaluation[\"id\"].str.replace(\"evaluation\", \"validation\")\n"
     ]
    }
   ],
   "source": [
    "make_submission(fcst.merge(zero_frq.rename(columns={\"id\":\"unique_id\"}), how=\"left\", on=[\"unique_id\"]).assign(yhat=lambda x : np.where(\n",
    "    (x[\"LGBMRegressor/BottomUp\"].clip(0, None) > 0) & (x[\"freq\"] >= 0.95), 0, x[\"LGBMRegressor/BottomUp\"].clip(0, None))).rename(columns={\"unique_id\":\"id\", \"ds\":\"date\"}), \n",
    "                fcst_col=\"yhat\", filename=\"v3hier\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "fcst",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "fcst",
   "language": "python",
   "name": "fcst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
