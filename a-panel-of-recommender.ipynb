{"cells":[{"metadata":{"_uuid":"611971c7a4dad964833760f98beb2b4112912989","_cell_guid":"d4dcc64a-5197-479c-b2e9-4117abd28228","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nfrom tqdm import tqdm\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity, linear_kernel\nimport scipy\nimport math\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm \n#Top-N accuracy metrics consts\nEVAL_RANDOM_SAMPLE_NON_INTERACTED_PROJECTS = 100\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport pandas as pd \nimport numpy as np\nfrom scipy.sparse import csr_matrix, csc_matrix, lil_matrix, coo_matrix, dok_matrix, vstack, hstack\n\n%matplotlib inline \nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_union\nfrom sklearn.base import BaseEstimator\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import euclidean_distances\nimport pickle\nimport string\nfrom sklearn.preprocessing import normalize\nimport timeit\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\nremove_list =  [\"The\", \"ANN\", \"A\", \"In\", \"two\", \"However\", \"life\", \"But\", \"MAL\", \n                \"AniDB\", \"named\", \"He\", \"find\", \"[Written\", \"Rewrite]\", \"become\", \"must\", \"this\", \"also\", \n                \"when\", \"as\", \"get\", \"ann\", \"no\", \"it\", \"It\", \"AniDB\", \"s\"] #hand typing\nremove_list2 =  [\"amount\", \"animes\", \"s\", \"no\", \"AniDB\", \"Source\", \"written mal rewrite\", \"mal\", \"Rewrite\", \"written\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73b87d93a25c91d921e355d4e3bcd2abc856e1dd","_kg_hide-input":true},"cell_type":"code","source":"\"\"\" i scrapped description of each anime for content recommender\"\"\"\nanime_path = \"../input/anime-description/AnimeProcessed.csv\"\n\ndef binning(col, cut_points, labels=None):\n    #Define min and max values:\n    minval = col.min()\n    maxval = col.max()\n    #create list by adding min and max to cut_points\n    break_points = [minval] + cut_points + [maxval]\n    #if no labels provided, use default labels 0 ... (n-1)\n    if not labels:\n        labels = range(len(cut_points)+1)\n    #Binning using cut function of pandas\n    colBin = pd.cut(col, bins=break_points, labels=labels, include_lowest=True)\n    return colBin\n\ndef create_soup(x):\n    return ''.join(x['desc']) + \"\\n\" +  ''.join(x[\"overall_rating\"]) + \"\\n\" + ''.join(x['name']) + \"\\n\" +''.join(x['genre'])  + \"\\n\" +''.join(x['type']) +\"\\n\" + ''.join(x[\"episodes\"]) +\"\\n\" + ''.join(x[\"members\"])\n\ndef load_anime(path) :\n    anime = pd.read_csv(anime_path, sep=\",\")\n    anime.rename(columns = {'animeid':'anime_id', \"rating\" : \"overall_rating\"}, inplace = True)\n    anime['desc'] =anime['desc'].map(str).apply(lambda x: \" \".join(x for x in x.split() if x not in (stop_words, remove_list)))\n    \n    anime[\"members\"] = binning(anime[\"members\"], [200000, 400000 ,600000],\n                               [\"seasonal\", \"normal\", \"popular\", \"mainstream\"])\n    \n    anime[\"episodes\"] = binning(anime[\"episodes\"], [13, 24 ,52],\n                                [\"short animes\", \"normal animes\", \"long animes\", \"mainstream animes\"])\n    \n    anime[\"overall_rating\"] = binning(anime[\"overall_rating\"], [3, 5 ,7],\n                                [\"bad\", \"neutral\", \"cool\", \"perfect\"])\n    \n    for col in [\"desc\", \"name\", \"genre\", \"type\", \"members\", \"episodes\", \"year\", \"overall_rating\"] :\n        anime[col] = anime[col].map(str)\n        \n    anime['soup'] = anime.apply(create_soup, axis =1)\n    anime['soup'] = anime[\"soup\"].apply(lambda x : re.sub('[^A-Za-z]+', ' ', str(x)))\n    anime['soup'] =anime['soup'].map(str).apply(lambda x: \" \".join(x for x in x.split() if x not in remove_list2))\n    return anime\n\nanime = load_anime(anime_path)\nALL_ANIMES = set(anime['anime_id'])\nprint(\"Shape of anime dataframe is {0}\".format(anime.shape))\nanime.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edfa43d9c7a2874a9bce81eea40f90200fe47221"},"cell_type":"code","source":"rating_path = \"../input/anime-recommendations-database/rating.csv\"\n\ndef user_rating_matrix():\n    URM = pd.read_csv(rating_path, sep=\",\")\n    URM.rating.replace({-1: np.nan}, regex=True, inplace = True)\n    URM.dropna(inplace = True)\n    animes = URM.anime_id.unique()\n    users = URM.user_id.unique()\n    n_animes = len(animes)\n    n_users =  len(users)\n    print(\"Shape of URM dataframe is {0}\".format(URM.shape))\n    print(\"There's {0} users and {1} animes\".format(str(n_users), str(n_animes)))\n    return URM\n\nURM = user_rating_matrix()\nURM.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb0abc9334cdfd6c312310b389890d8b584a3403","trusted":true},"cell_type":"code","source":"def merge_it_all(df1, df2, n_sample) :\n    df = pd.merge(df1, df2, left_on =\"anime_id\", right_on = \"anime_id\")\n    df = df[df[\"user_id\"] <= n_sample]\n    return df\n\ndf = merge_it_all(anime, URM, 6000)\n\ndef get_unbiased_rating(df) : \n    users_interactions_count_df = df.groupby(['user_id', 'anime_id']).size().groupby('user_id').size()\n    print('# users: %d' % len(users_interactions_count_df))\n    users_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df >= 8].reset_index()[['user_id']]\n    print('# users with at least 8 interactions: %d' % len(users_with_enough_interactions_df))\n    print('# of interactions: %d' % len(df))\n    interactions_from_selected_users_df = df.merge(users_with_enough_interactions_df, \n               how = 'right',\n               left_on = 'user_id',\n               right_on = 'user_id')\n    print('# of interactions from users with at least 8 interactions: %d' % len(interactions_from_selected_users_df))\n    popularity = interactions_from_selected_users_df.groupby('anime_id').size().reset_index(name='popularity')\n    interactions_full_df = pd.merge(popularity, interactions_from_selected_users_df)\n    interactions_full_df.rename({\"rating\" : \"user_rating\"}, inplace = True, axis = 1)\n    return interactions_full_df, interactions_from_selected_users_df\n\ninteractions_full_df, interactions_from_selected_users_df = get_unbiased_rating(df)\ninteractions_full_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9138dac5b609260d7275c0ec1809c838c02f5b14"},"cell_type":"code","source":"interactions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n                                   stratify=interactions_full_df['user_id'], \n                                   test_size=0.20,\n                                   random_state=42)\n\nprint('# interactions on Train set: %d' % len(interactions_train_df))\nprint('# interactions on Test set: %d' % len(interactions_test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb43763baba62cb42b969614f298a57bc033d0be"},"cell_type":"code","source":"#Indexing by personId to speed up the searches during evaluation\ninteractions_full_indexed_df = interactions_full_df.set_index('user_id')\ninteractions_train_indexed_df = interactions_train_df.set_index('user_id')\ninteractions_test_indexed_df = interactions_test_df.set_index('user_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e7b4b34941645e194f8eeff275b46a959111fa8"},"cell_type":"code","source":"ALL_ANIMES = set(anime['anime_id'])\n\ndef get_anime_watched(user_id, interactions_df):\n    try:\n        anime_rated = interactions_df.loc[user_id]['anime_id']\n        return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n    except KeyError:\n        return []\n    \ndef get_not_watched_anime_sample(user_id, sample_size, seed=42):\n        anime_watched = get_anime_watched(user_id, interactions_full_indexed_df)\n        non_watched_anime = ALL_ANIMES - anime_watched\n        non_watched_anime_sample = random.sample(non_watched_anime, sample_size)\n        return set(non_watched_anime_sample)\n\ndef _verify_hit_top_n(anime_id, recommended_anime, topn):        \n        try:\n            index = next(i for i, c in enumerate(recommended_anime) if c == anime_id)\n        except:\n            index = -1\n        hit = int(index in range(0, topn))\n        return hit, index\n    \ndef evaluate_model_for_anime_watcher(model, user_id):\n    \n    anime_watched_values_testset = interactions_test_indexed_df.loc[user_id]\n    if type(anime_watched_values_testset['anime_id']) == pd.Series:\n        user_anime_watched_testset = set(anime_watched_values_testset['anime_id'])\n    else:\n        user_anime_watched_testset = set([anime_watched_values_testset['anime_id']])  \n    user_anime_watched_count_testset = len(user_anime_watched_testset) \n\n    #Getting a ranked recommendation list from a model for a given donor\n    user_recs_df = model.recommend_anime(user_id,\n                                         anime_to_ignore=get_anime_watched(\n                                             user_id,\n                                             interactions_train_indexed_df),\n                                         topn=100000000)\n    \n    hits_at_3_count = 0\n    hits_at_5_count = 0\n    hits_at_10_count = 0\n    #For each anime seen by user \n    for anime_id in user_anime_watched_testset:\n        #Getting a random sample (100) non watched anime \n        non_watched_anime_sample = get_not_watched_anime_sample(user_id,\n                                                            sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_PROJECTS,\n                                                            seed=42)\n        #Combining the current anime with the 100 random anime\n        anime_to_filter_recs = non_watched_anime_sample.union(set([anime_id]))\n        #Filtering only recommendations that are either the donated project or from a random sample of 100 non-donated projects\n        valid_recs_df = user_recs_df[user_recs_df['anime_id'].isin(anime_to_filter_recs)]                    \n        valid_recs = valid_recs_df['anime_id'].values\n        \n        #Verifying if the current anime is among the Top-N recommended projects\n        hit_at_3, index_at_3 = _verify_hit_top_n(anime_id, valid_recs, 3)\n        hits_at_3_count += hit_at_3\n        hit_at_5, index_at_5 = _verify_hit_top_n(anime_id, valid_recs, 5)\n        hits_at_5_count += hit_at_5\n        hit_at_10, index_at_10 = _verify_hit_top_n(anime_id, valid_recs, 10)\n        hits_at_10_count += hit_at_10\n        \n    recall_at_3 = hits_at_3_count / float(user_anime_watched_count_testset)\n    recall_at_5 = hits_at_5_count / float(user_anime_watched_count_testset)\n    recall_at_10 = hits_at_10_count / float(user_anime_watched_count_testset)\n\n    anime_metrics = {'hits@3_count':hits_at_3_count, \n                     'hits@5_count':hits_at_5_count, \n                      'hits@10_count':hits_at_10_count, \n                      'watched_count': user_anime_watched_count_testset,\n                      'recall@3': recall_at_3,\n                      'recall@5': recall_at_5,\n                      'recall@10': recall_at_10}\n    return anime_metrics","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4a2383cdc3a84d4627317967b842263870d47414"},"cell_type":"code","source":"def evaluate_model(model):\n    v = tqdm(enumerate(list(interactions_test_indexed_df.index.unique().values)))\n    anime_user_metrics = [{**evaluate_model_for_anime_watcher(model, user_id), '_user_id': user_id} \n                          for idx, user_id in v]\n    \n    detailed_results_df = pd.DataFrame(anime_user_metrics) \\\n                        .sort_values('watched_count', ascending=False)\n        \n    global_recall_at_3 = detailed_results_df['hits@3_count'].sum() / float(detailed_results_df['watched_count'].sum())\n    global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['watched_count'].sum())\n    global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['watched_count'].sum())\n\n    global_metrics = {'modelName': model.get_model_name(),\n                      'recall@3': global_recall_at_3,\n                      'recall@5': global_recall_at_5,\n                      'recall@10': global_recall_at_10}   \n\n    return global_metrics, detailed_results_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"211749bcd60d3c05d990d097941e498f5ac7c4aa"},"cell_type":"markdown","source":"# Popularity "},{"metadata":{"trusted":true,"_uuid":"8a91b34ab372fb8ad9802fb8454f3fa4204fc1cb"},"cell_type":"code","source":"item_popularity_df = interactions_full_df.groupby('anime_id')['user_rating'].sum().sort_values(ascending=False).reset_index()\n\nclass PopularityRecommender:\n    \n    MODEL_NAME = 'Popularity'\n    \n    def __init__(self, popularity_df, anime_df=None):\n        self.popularity_df = popularity_df\n        self.anime_df = anime_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n    \n    def get_seen(self, user_id, anime_df):\n        try:\n            anime_rated = self.anime_df.loc[self.anime_df[\"user_id\"] == user_id]['anime_id']\n            return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n        except KeyError:\n            return []\n        \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10, verbose=False):\n        # Recommend the more popular items that the user hasn't seen yet.\n        recommendations_df = self.popularity_df[~self.popularity_df['anime_id'].isin(self.get_seen(user_id, self.anime_df))] \\\n                               .sort_values('user_rating', ascending = False) \\\n                               .head(topn)\n\n        if verbose:\n            if self.anime_df is None:\n                raise Exception('\"items_df\" is required in verbose mode')\n\n            recommendations_df = recommendations_df.merge(self.anime_df, how = 'left', \n                                                          left_on = 'anime_id', \n                                                          right_on = 'anime_id')[['rating', \n                                                                                  'anime_id',\n                                                                                  'name', \n                                                                                  'type', \n                                                                                  'genre']]\n        return recommendations_df\n\npopularity_model = PopularityRecommender(item_popularity_df, anime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ece0377cd1c57f73b07f3e84c7dd63e0d9f3387"},"cell_type":"code","source":"\"\"\"print('Evaluating Popularity recommendation model...')\npop_global_metrics, pop_detailed_results_df = evaluate_model(popularity_model)\nprint('\\nGlobal metrics:\\n%s' % pop_global_metrics)\npop_detailed_results_df.head(10)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d357985632cf1f58a0f37c60eb698ccc0f90deb"},"cell_type":"markdown","source":"# content based"},{"metadata":{"trusted":true,"_uuid":"3aa64815edf54293f305a58e9aa508d4cd77e840"},"cell_type":"code","source":"anime_ids = anime['anime_id'].tolist()\n\nclass Users_Profiler : \n    \n    def __init__(self, connector_matrix, anime_ids):\n        self.connector_matrix = connector_matrix #base for cosine \n        self.anime_ids = anime_ids\n        \n    def get_anime_profile(self, anime_id):\n        idx = self.anime_ids.index(anime_id)\n        anime_profile = self.connector_matrix[idx:idx+1]\n        return anime_profile\n    \n    def get_item_profiles(self, ids):\n        item_profiles_list = [self.get_anime_profile(x) for x in np.ravel([ids])]\n        item_profiles = vstack(item_profiles_list)\n        return item_profiles\n    \n    def build_users_profile(self, user_id, interactions_indexed_df):\n        interactions_anime_df = interactions_indexed_df.loc[user_id]\n        user_item_profiles = self.get_item_profiles(interactions_anime_df['anime_id'])\n        user_item_strengths = np.array(interactions_anime_df['user_rating']).reshape(-1,1) \n        #Weighted average of item profiles by the interactions strength\n        user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths),\n                                                  axis=0) / (np.sum(user_item_strengths)+1) #+1 added\n\n        user_profile_norm = sklearn.preprocessing.normalize(user_item_strengths_weighted_avg)\n        return user_profile_norm\n    \n    def build_users_profiles(self): \n        interactions_indexed_df = interactions_full_df[interactions_full_df['anime_id'].isin(anime['anime_id'])].set_index('user_id')\n        user_profiles = {}\n        for user_id in tqdm(interactions_indexed_df.index.unique()):\n            user_profiles[user_id] = self.build_users_profile(user_id, interactions_indexed_df)\n        return user_profiles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ccfb48202cf6aeb4d22ac501eba6927e28d6336"},"cell_type":"code","source":"\"\"\" Initialize vectorizer\"\"\"\n    \nword_vectorizer = TfidfVectorizer(ngram_range =(1,4),\n                             min_df=5, max_df=0.9,\n                             strip_accents='unicode',\n                             stop_words = 'english',\n                             analyzer = 'word',\n                             use_idf=1,\n                             smooth_idf=1,\n                             sublinear_tf=1)\n\ntfidf_matrix = word_vectorizer.fit_transform(anime['soup'])\ntfidf_feature_names = word_vectorizer.get_feature_names()\n\n\"\"\" compute users profiles \"\"\"\nU2P = Users_Profiler(tfidf_matrix, anime_ids)\nU2tfifd = U2P.build_users_profiles()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49300cfe9b8b6b7c07bdb9ab078d1117f3a4da74"},"cell_type":"code","source":"class BaseRecommender:\n    \n    MODEL_NAME = 'BaseforCosine'\n    \n    def __init__(self, users_profile, connector_matrix, anime_df=None):\n        self.anime_ids = anime_ids\n        self.anime_df = anime_df\n        self.users_profile=  users_profile\n        self.connector_matrix = connector_matrix\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def _get_similar_anime_to_users_profile(self, user_id, topn=1000):\n        #Computes the cosine similarity between the donor profile and all project profiles\n        cosine_similarities = cosine_similarity(self.users_profile[user_id], self.connector_matrix)\n        #Gets the top similar projects\n        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n        #Sort the similar projects by similarity\n        similar_anime = sorted([(anime_ids[i], cosine_similarities[0,i]) for i in similar_indices],\n                                  key=lambda x: -x[1])\n        return similar_anime\n    \n    def get_seen(self, user_id, anime_df):\n        try:\n            anime_rated = self.anime_df.loc[self.anime_df[\"user_id\"] == user_id]['anime_id']\n            return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n        except KeyError:\n            return []\n        \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10, verbose=False):\n        similar_anime = self._get_similar_anime_to_users_profile(user_id)\n        #Ignores projects the donor has already donated\n        similar_anime_filtered = list(filter(lambda x: x[0] not in self.get_seen(user_id, \n                                                                            self.anime_df), \n                                             similar_anime))\n        \n        recommendations_df = pd.DataFrame(similar_anime_filtered, columns=['anime_id', 'user_rating']).head(topn)\n\n        recommendations_df = recommendations_df.merge(self.anime_df, how = 'left', \n                                                    left_on = 'anime_id', \n                                                    right_on = 'anime_id')[['anime_id', 'user_rating', 'name', 'genre']].drop_duplicates()\n\n\n        return recommendations_df\n\nContentBasedmodel = BaseRecommender(U2tfifd, tfidf_matrix, anime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31a582748d0058d82c3e3c9ca5d463dba356f69a"},"cell_type":"code","source":"ContentBasedmodel.recommend_anime(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c723780464b42ba517ade6fdef5f0f74c72dfb7b"},"cell_type":"code","source":"print('Evaluating Content-Based Filtering model...')\ncb_global_metrics, cb_detailed_results_df = evaluate_model(ContentBasedmodel)\nprint('\\nGlobal metrics:\\n%s' % cb_global_metrics)\ncb_detailed_results_df = cb_detailed_results_df[['_user_id', 'watched_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                                'recall@3','recall@5','recall@10']]\ncb_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57dd1a57fa18894bd3213f9359b467d913e4c8ef"},"cell_type":"markdown","source":"# WORD EMBEDDING"},{"metadata":{"trusted":true,"_uuid":"73aa216d4c784131b949d107076e1fe3daf5faad"},"cell_type":"code","source":"def generate_doc_vectors(s):\n    words = str(s).lower().split() \n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        if w in embeddings_index:\n            M.append(embeddings_index[w])\n    v = np.array(M).sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())\n\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\nembeddings_index = {}\nf = open(EMBEDDING_FILE, encoding=\"utf8\")\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n        \nxtrain = anime[\"soup\"].values\nxtrain_embeddings = [generate_doc_vectors(x) for x in tqdm(xtrain)]\nfrom sklearn.preprocessing import StandardScaler\nsc= StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f5055ad37cfad944cc4fe4dbd0bb5f96702b2d"},"cell_type":"code","source":"xtrain_embeddings = sc.fit_transform(xtrain_embeddings)\nxtrain_embeddings = csr_matrix(xtrain_embeddings)\nU2f = Users_Profiler(xtrain_embeddings, anime_ids)\nU2fastt = U2f.build_users_profiles()\nfastcontent = BaseRecommender(U2fastt, xtrain_embeddings, anime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d42488ac5690d78386d5047bfebde3b89e1713a5"},"cell_type":"code","source":"print('Evaluating Content-Based Filtering model...')\nfglobal_metrics, fdetailed_results_df = evaluate_model(fastcontent)\nprint('\\nGlobal metrics:\\n%s' % fglobal_metrics)\nfdetailed_results_df = fdetailed_results_df[['_user_id', 'watched_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                                'recall@3','recall@5','recall@10']]\nfdetailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77757f480e1a65ba3fe9d2b53f1074f10f20bff4"},"cell_type":"markdown","source":"pretrained word embedding matrix seems not very efficient for content recommender compared to tfifd"},{"metadata":{"_uuid":"221311357770c6af21acd5242441f3e6b801f966"},"cell_type":"markdown","source":"## KNN based "},{"metadata":{"trusted":true,"_uuid":"36ca03a80cb0df8340a684dcdf2fb6c026071f05"},"cell_type":"code","source":"anime_features = pd.concat([anime[\"genre\"].str.get_dummies(sep=\",\"),\n                            pd.get_dummies(anime[[\"type\"]]),\n                            pd.get_dummies(anime[[\"episodes\"]]),\n                            pd.get_dummies(anime[[\"members\"]]),\n                            pd.get_dummies(anime[[\"overall_rating\"]])], axis=1)\n\nanime_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"665bf30605deed9080d326782ed583bb2d826042"},"cell_type":"code","source":"anime_ids = anime[\"anime_id\"].tolist()\nfrom sklearn.preprocessing import MaxAbsScaler\nmabs = MaxAbsScaler()\nanime_features_scale = mabs.fit_transform(anime_features)\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=6, algorithm='brute', metric = \"cosine\").fit(anime_features_scale)\ndistances, indices = nbrs.kneighbors(anime_features_scale)\nfrom scipy.sparse import csr_matrix\nindices =  csr_matrix(indices)\ndistances =  csr_matrix(distances)\n\nU2KNN = Users_Profiler(distances, anime_ids)\nKNN_model = U2KNN.build_users_profiles()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33b5549a2c807b4d90384308512c20544d3f707b"},"cell_type":"code","source":"KNN_RECO = BaseRecommender(KNN_model, distances, anime)\nprint('Evaluating Content-Based Filtering model...')\nglobal_metrics, detailed_results_df = evaluate_model(KNN_RECO)\nprint('\\nGlobal metrics:\\n%s' % global_metrics)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da9d91e0427f76149ea16826746bc54c9e167dea"},"cell_type":"markdown","source":"# MF"},{"metadata":{"trusted":true,"_uuid":"c5961e5cf2c08c4fc9d2385ba040ec4349b21b06"},"cell_type":"code","source":"pivpiv = interactions_full_df.pivot(index = 'user_id', columns ='anime_id', values = 'user_rating')\nR= np.array(interactions_full_df.pivot(index = 'user_id', columns ='anime_id', values = 'user_rating').fillna(0))\niddd = pivpiv.index.values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2401421ef0e78068cc9987fee602931da57cff17"},"cell_type":"code","source":"class MF():\n\n    # Initializing the user-movie rating matrix, no. of latent features, alpha and beta.\n    def __init__(self, R, K, alpha, beta, iterations):\n        self.R = R\n        self.num_users, self.num_items = R.shape\n        self.K = K\n        self.alpha = alpha\n        self.beta = beta\n        self.iterations = iterations\n\n        \"\"\" R – The user-anime rating matrix\n            K – Number of latent features\n            alpha – Learning rate for stochastic gradient descent\n            beta – Regularization parameter for bias\n            iterations – Number of iterations to perform stochastic gradient descent\"\"\"\n    # Initializing user-feature and movie-feature matrix \n    def train(self):\n        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n\n        # Initializing the bias terms\n        self.b_u = np.zeros(self.num_users)\n        self.b_i = np.zeros(self.num_items)\n        self.b = np.mean(self.R[np.where(self.R != 0)])\n\n        # List of training samples\n        self.samples = [\n        (i, j, self.R[i, j])\n        for i in range(self.num_users)\n        for j in range(self.num_items)\n        if self.R[i, j] > 0\n        ]\n\n        # Stochastic gradient descent for given number of iterations\n        training_process = []\n        for i in tqdm(range(self.iterations)):\n            np.random.shuffle(self.samples)\n            self.sgd()\n            mse = self.mse()\n            training_process.append((i, mse))\n            if (i+1) % 5 == 0:\n                print(\"Iteration: %d ; mse = %.4f\" % (i+1, mse))\n\n        return training_process\n\n    # Computing total mean squared error\n    def mse(self):\n        xs, ys = self.R.nonzero()\n        predicted = self.full_matrix()\n        error = 0\n        for x, y in zip(xs, ys):\n            error += pow(self.R[x, y] - predicted[x, y], 2)\n        return np.sqrt(error)\n\n    # Stochastic gradient descent to get optimized P and Q matrix\n    def sgd(self):\n        for i, j, r in self.samples:\n            prediction = self.get_rating(i, j)\n            e = (r - prediction)\n\n            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n\n            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n\n    # Ratings for user i and moive j\n    def get_rating(self, i, j):\n        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n        return prediction\n\n    # Full user-movie rating matrix\n    def full_matrix(self):\n        return mf.b + mf.b_u[:,np.newaxis] + mf.b_i[np.newaxis:,] + mf.P.dot(mf.Q.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dd1f524a3091db8f597fb84b5333a1081bd46ac"},"cell_type":"code","source":"mf = MF(R, K=200, alpha=0.009999, beta=0.01, iterations=120)\ntraining_process = mf.train()\npred_df = mf.full_matrix()\ncf_predictions_df = pd.DataFrame(pred_df, columns = pivpiv.columns, index=iddd).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05f927f3727cee84267abe1ac01687471a0bce24"},"cell_type":"code","source":"class CF_SG_Recommender:\n    \n    MODEL_NAME = 'Collaborative Filtering With Gradiant Descent'\n    \n    def __init__(self, cf_predictions_df, anime_df=None): #anime_df = keep_full_info\n        self.cf_predictions_df = cf_predictions_df\n        self.anime_df = anime_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n    \n    def get_seen(self, user_id, anime_df):\n        try:\n            anime_rated = self.anime_df.loc[self.anime_df[\"user_id\"] == user_id]['anime_id']\n            return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n        except KeyError:\n            return []\n        \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10):\n        # Get and sort the donor's predictions\n        sorted_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False) \\\n                                    .reset_index().rename(columns={user_id: 'recStrength'})\n\n        # Recommend the highest predicted projects that the donor hasn't donated to\n        recommendations_df = sorted_predictions[~sorted_predictions['anime_id'].isin(\n            self.get_seen(user_id, self.anime_df))] \\\n        .sort_values('recStrength', ascending = False) \\\n        .head(topn)\n                \n        recommendations_df = recommendations_df.merge(self.anime_df, how = 'left', \n                                                      left_on = 'anime_id', \n                                                      right_on = \"anime_id\")[['recStrength',\n                                                                              'anime_id',]].drop_duplicates()\n\n        return recommendations_df\n    \ncfrsgd_model = CF_SG_Recommender(cf_predictions_df, interactions_full_df)\nprint('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')\ncfrsgd_global_metrics, cfrsgd_detailed_results_df = evaluate_model(cfrsgd_model)\nprint('\\nGlobal metrics:\\n%s' % cfrsgd_global_metrics)\ncfrsgd_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"545fcc7e2dd80008c932939483b9335c10d22421"},"cell_type":"markdown","source":"# SVD"},{"metadata":{"trusted":true,"_uuid":"00acd3903b3d975a2cebfbdb70054f61ad9144b0"},"cell_type":"code","source":"U, sigma, Vt = svds(R, k= 450)\nsigma = np.diag(sigma)\npredicted_ratings = np.dot(np.dot(U, sigma), Vt) \ncf_preds_df = pd.DataFrame(predicted_ratings, \n                           columns = pivpiv.columns, \n                           index=iddd).transpose()\n\nfrom math import sqrt \n\nprint(\"RMSE is {0}\".format(sqrt(mean_squared_error(R, predicted_ratings))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31dc38d22dbc16e96fcfc0a52118f8b444f0f40e"},"cell_type":"code","source":"class CFRecommender:\n    \n    MODEL_NAME = 'Collaborative Filtering '\n    \n    def __init__(self, cf_predictions_df, anime_df=None): #anime_df = keep_full_info\n        self.cf_predictions_df = cf_predictions_df\n        self.anime_df = anime_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n    \n    def get_seen(self, user_id, anime_df):\n        try:\n            anime_rated = self.anime_df.loc[self.anime_df[\"user_id\"] == user_id]['anime_id']\n            return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n        except KeyError:\n            return []\n        \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10):\n        # Get and sort the donor's predictions\n        sorted_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False) \\\n                                    .reset_index().rename(columns={user_id: 'recStrength'})\n\n        # Recommend the highest predicted projects that the donor hasn't donated to\n        recommendations_df = sorted_predictions[~sorted_predictions['anime_id'].isin(self.get_seen(user_id, self.anime_df))] \\\n        .sort_values('recStrength', ascending = False) \\\n        .head(topn)\n                \n        recommendations_df = recommendations_df.merge(self.anime_df, how = 'left', \n                                                      left_on = 'anime_id', \n                                                      right_on = \"anime_id\")[['recStrength',\n                                                                              'anime_id', 'name']].drop_duplicates()\n\n        return recommendations_df\n    \ncf_model = CFRecommender(cf_preds_df, anime)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6a5de836712e25587d6c231a817a380a1dc264"},"cell_type":"markdown","source":"# Hybrid "},{"metadata":{"trusted":true,"_uuid":"c2d2dfa23536565c72eb06b2e0692d3af92fe84f"},"cell_type":"code","source":"class HybridRecommender:\n    \n    MODEL_NAME = 'Hybrid'\n    \n    def __init__(self, cb_rec_model, cf_model, anime_df):\n        self.cb_rec_model = cb_rec_model\n        self.cf_model = cf_model\n        self.anime_df = anime_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10):\n        #Getting the top-1000 Content-based filtering recommendations\n        cb_recs_df = self.cb_rec_model.recommend_anime(user_id,topn=1000)\\\n        .rename(columns={'user_rating': 'recStrengthCB'})\n        \n        #Getting the top-1000 Collaborative filtering recommendations\n        cf_recs_df = self.cf_model.recommend_anime(user_id, topn=1000)\\\n        .rename(columns={'recStrength': 'recStrengthCF'})\n        \n        #Combining the results by Project ID\n        recs_df = cb_recs_df.merge(cf_recs_df,\n                                   how = 'inner', \n                                   left_on = 'anime_id', \n                                   right_on = 'anime_id')\n        \n        #Computing a hybrid recommendation score based on CF and CB scores\n        recs_df['recStrengthHybrid'] = recs_df['recStrengthCB'] * recs_df['recStrengthCF']\n        \n        #Sorting recommendations by hybrid score\n        recommendations_df = recs_df.sort_values('recStrengthHybrid', ascending=False).head(topn)\n\n        recommendations_df = recommendations_df.merge(self.anime_df, how = 'left', \n                                                    left_on = 'anime_id', \n                                                    right_on = 'anime_id')[['recStrengthHybrid',\n                                                                            'anime_id',\n                                                                            \"name\"]]\n\n\n        return recommendations_df\n    \nhybrid_model = HybridRecommender(ContentBasedmodel, cf_model, anime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a843859c8fddd1be8371c2c1db8019bd6946e695"},"cell_type":"code","source":"print('Evaluating hybrid_model...')\nhybrid_global_metrics, hybrid_detailed_results_df = evaluate_model(hybrid_model)\nprint('\\nGlobal metrics:\\n%s' % hybrid_global_metrics)\nhybrid_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56473c1ea51ce9912d3951b1d16808180b0f6e11"},"cell_type":"markdown","source":"# Deep Learning"},{"metadata":{"trusted":true,"_uuid":"91a393bb4e2a989dd573c62634e8b8372ff5276b"},"cell_type":"code","source":"import keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Reshape, merge\nfrom keras.layers import concatenate, dot\nfrom keras.layers.core import Flatten, Dense, Dropout, Lambda\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"365f02a93bd1f69c6e9a26a08143d2ea88f83beb"},"cell_type":"code","source":"users = interactions_full_df.user_id.unique()\nanimes = interactions_full_df.anime_id.unique()\nn_users = len(users)\nn_animes =  len(animes)\n\nuserid2idx = {o:i for i,o in enumerate(users)}\nanimesid2idx = {o:i for i,o in enumerate(animes)}\ninteractions_full_df['user_id'] = interactions_full_df['user_id'].apply(lambda x: userid2idx[x])\ninteractions_full_df['anime_id'] = interactions_full_df['anime_id'].apply(lambda x: animesid2idx[x])\n#interactions_from_selected_users_df.rename(columns={\"rating\": \"user_rating\"}, inplace = True)\n\ntrain, valid = train_test_split(interactions_full_df,\n                                   stratify=interactions_full_df['user_id'], \n                                   test_size=0.20,\n                                   random_state=42)\n\ninteractions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n                                   stratify=interactions_full_df['user_id'], \n                                   test_size=0.20,\n                                   random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a94b7468e55c8c31470045a69432fa2e6c74322"},"cell_type":"code","source":"n_latent_factors_user = 150\nn_latent_factors_animes = 500\nn_latent_factors_mf = 3\n\n\"\"\" Anime MultiLayer Perceptron \"\"\"\nanimes_input = Input(shape=[1],name='Item')\nanimes_embedding_mlp = Embedding(n_animes + 1, \n                                 n_latent_factors_animes, \n                                 name='Animes-Embedding-MLP')(animes_input)\nanimes_vec_mlp = Flatten(name='FlattenAnimes-MLP')(animes_embedding_mlp)\n\n\"\"\" Anime embedding for matrix factorisation \"\"\"\nanimes_embedding_mf = Embedding(n_animes + 1,\n                                n_latent_factors_mf,\n                                name='Animes-Embedding-MF')(animes_input)\nanimes_vec_mf = Flatten(name='FlattenAnimes-MF')(animes_embedding_mf)\n\n\"\"\" User MultiLayer Perceptron \"\"\"\n\nuser_input = Input(shape=[1],name='User')\nuser_vec_mlp = Flatten(name='FlattenUsers-MLP')(Embedding(n_users + 1,\n                                                          n_latent_factors_user,\n                                                          name='User-Embedding-MLP')(user_input))\nuser_vec_mlp = Dropout(0.2)(user_vec_mlp)\n\n\"\"\" Users embedding for matrix factorisation \"\"\"\nuser_vec_mf = Flatten(name='FlattenUsers-MF')(Embedding(n_users + 1, \n                                                        n_latent_factors_mf,\n                                                        name='User-Embedding-MF')(user_input))\nuser_vec_mf = Dropout(0.2)(user_vec_mf)\n\nconcat = concatenate([animes_vec_mlp, user_vec_mlp])\nconcat_dropout = Dropout(0.2)(concat)\ndense = Dense(200,name='FullyConnected')(concat_dropout)\ndense_batch = BatchNormalization(name='Batch')(dense)\ndense_2 = Dense(100,name='FullyConnected-1')(dense_batch)\ndense_batch_2 = BatchNormalization(name='Batch-2')(dense_2)\ndense_3 = Dense(50,name='FullyConnected-2')(dense_batch_2)\ndense_4 = Dense(20,name='FullyConnected-3', activation='relu')(dense_3)\n\npred_mf = dot([animes_vec_mf, user_vec_mf], axes = -1, name='Dot')\npred_mlp = Dense(1, activation='relu',name='Activation')(dense_4)\n\ncombine_mlp_mf = concatenate([pred_mf, pred_mlp],name='Concat-MF-MLP')\nresult_combine = Dense(100,name='Combine-MF-MLP')(combine_mlp_mf)\ndeep_combine = Dense(100,name='FullyConnected-4')(result_combine)\n\nresult = Dense(1,name='Prediction')(deep_combine)\nDeepCF = Model([user_input, animes_input], result)\nopt = Adam(lr =0.09)\nDeepCF.compile(optimizer=opt ,loss= 'mean_absolute_error')\nDeepCF.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fc53e21a5fe0ee4e7a662bac3846cba7fc9cdc"},"cell_type":"code","source":"history = DeepCF.fit([train.user_id, train.anime_id], train.user_rating, batch_size=512, epochs=5, \n          validation_data=([valid.user_id, valid.anime_id], \n            valid.user_rating))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce3bda31506a6d94b1057a1ba4d1467738fd78ba"},"cell_type":"code","source":"import math\nmin_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\nprint ('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a38db723f2eb54491f5c53cc454e8212f40f311"},"cell_type":"code","source":"user_map = {}\nanime_map = {}\n\ndef getID(val, tag):\n    return tags_dict[tag][val]\n\ndef mapping() : \n    print(\"start ...\")\n    unique_users = list(interactions_full_df['user_id'].value_counts().index)\n    unique_animes = list(interactions_full_df['anime_id'].value_counts().index)\n    for i, user in enumerate(unique_users):\n        user_map[user] = i+1\n    for i, anime in enumerate(unique_animes):\n        anime_map[anime] = i+1\n    tags_dict = {'users' : user_map, 'anime' : anime_map}\n    print(\"mapping done ...\")\n    return tags_dict\n\ntags_dict = mapping()\n\ninteractions_full_df['ani_id'] = interactions_full_df['user_id'].apply(lambda x : getID(x, 'users'))\ninteractions_full_df['user_idd'] = interactions_full_df['anime_id'].apply(lambda x : getID(x, 'anime'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8bf6e00be9a5b1fb9e875d53cd36526dc429675"},"cell_type":"code","source":"class DeepRecommender:\n    \n    MODEL_NAME = 'PredictedRating_DeepRec'\n    \n    def __init__(self, deep_model, anime_users_df=None):\n        self.deep_model = deep_model\n        self.anime_users_df = anime_users_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n    \n    def get_seen(self, user_id, anime_users_df):\n        try:\n            anime_rated = self.anime_users_df.loc[self.anime_users_df[\"user_id\"] == user_id]['anime_id']\n            return set(anime_rated if type(anime_rated) == pd.Series else [anime_rated])\n        except KeyError:\n            return []\n    \n    def rate(self, deep_model, user_id, anime_id):\n        return deep_model.predict([np.array([user_id]), np.array([anime_id])])[0][0]\n    \n    def predict_rating(self, anime_id, userid):\n        return self.rate(DeepCF, anime_id - 1, userid - 1)\n    \n    def build_rec_df(self, user_id) : \n        print(\"starting to build rec df ...\")\n        user_ratings = self.anime_users_df.loc[self.anime_users_df['ani_id'] == user_id][['user_idd', 'ani_id', 'user_rating']]\n        user_ratings['predicted_amt'] = user_ratings.apply(lambda x: self.predict_rating(user_id, x['user_idd']), axis=1)\n        recommendations = self.anime_users_df.loc[self.anime_users_df['user_idd'].isin(user_ratings['user_idd']) == False][['user_idd', \"anime_id\"]]\\\n        .drop_duplicates()\n        recommendations['Predicted_ratings'] = recommendations.apply(lambda x: self.predict_rating(user_id, x['user_idd']), axis=1)\n        print(\"done ...\")\n        return recommendations\n    \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10, verbose=False):\n        print(\"recommendations ...\")\n        recommendations =  self.build_rec_df(user_id)\n        # Recommend the more popular items that the user hasn't seen yet.\n        recommendations_df = recommendations[~recommendations['anime_id'].isin(self.get_seen(user_id, self.anime_users_df))] \\\n        .sort_values('Predicted_ratings', ascending = False) \\\n        .head(topn)\n\n        if verbose:\n            if self.anime_users_df is None:\n                raise Exception('\"anime_users_df\" is required in verbose mode')\n\n            recommendations_df = pd.merge(recommendations_df, self.anime_users_df, how = 'right', \n                                                          left_on = 'anime_id', \n                                                          right_on = 'anime_id').drop_duplicates()\n        \n        return recommendations_df\n    \n    \ndeep_colab_model= DeepRecommender(DeepCF, interactions_full_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48d090456b570371808b041159b1e8a5fb5eae58"},"cell_type":"markdown","source":"I'll not trying to evaluate this model coz it tooks a while something like ~20h for 5k users the predict method is a bit slow."},{"metadata":{"_uuid":"83f6d9cf8cba0fd4e020e4cd08677f362320b5f6"},"cell_type":"markdown","source":"# **Draw a try with a user with fews rated anime for easier comparison"},{"metadata":{"trusted":true,"_uuid":"b3ce3e8a6c747d600bd9aa839bb885f72a3cc42a"},"cell_type":"code","source":"print(pop_rec.__len__(), cbr_rec.__len__(), cf_rec.__len__(), hybrid_rec.__len__(), embed_rec.__len__(), deep_colab_rec.__len__(), deep_rec.__len__(), mf_rec.__len__(), KNN_rec.__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"797e8fdb395879432641580774546944347a99f5"},"cell_type":"code","source":"def comparison(user, top_n) : \n    anime_list = get_anime_watched(user, interactions_full_df.set_index(\"user_id\"))\n    rated = anime[anime[\"anime_id\"].isin(anime_list)][\"name\"].tolist()\n\n    pop_rec = popularity_model.recommend_anime(user).head(top_n)\n    pop_rec = pd.merge(anime, pop_rec, how =  \"right\", left_on = \"anime_id\",right_on = \"anime_id\")[[\"name\"]]\n    pop_rec = list(pop_rec.name)\n    cbr_rec = ContentBasedmodel.recommend_anime(user).head(top_n)[\"name\"].tolist()\n    cf_rec = cf_model.recommend_anime(user).head(top_n)[\"name\"].tolist()\n    hybrid_rec = hybrid_model.recommend_anime(user).head(top_n)[\"name\"].tolist()\n    embed_rec = fastcontent.recommend_anime(user).head(top_n)[\"name\"].tolist()\n    deep_colab_rec = deep_colab_model.recommend_anime(user)\n    deep_rec = pd.merge(anime, deep_colab_rec, how =  \"right\",  left_on = \"anime_id\",\n                        right_on = \"anime_id\")[[\"name\"]].fillna(\"None\")\n    deep_rec = list(deep_rec.name[:top_n])\n    mf_rec = cfrsgd_model.recommend_anime(user).head(top_n)\n    mf_rec = pd.merge(anime, mf_rec,how =  \"right\",  left_on = \"anime_id\",right_on = \"anime_id\")[[\"name\"]]\n    mf_rec = list(mf_rec.name)\n    KNN_rec = KNN_RECO.recommend_anime(user).head(top_n)[\"name\"].tolist()\n\n    d = {'Popularity': pop_rec,\n         'KNN-Based' : KNN_rec,\n         'Deep_Content_based' : embed_rec,\n         'Content-Based': cbr_rec,\n         'Collaborative-Filtering': cf_rec,\n         'Hybrid': hybrid_rec,\n         'Deep Collaborative-Filtering' :  deep_rec, \n         \"Matrix Factorization\" : mf_rec}\n\n    d = pd.DataFrame(data=d)\n    return rated, d\n\nrated, compare = comparison(25, 5)\n\nprint(\"User has seen ... :\")\nfor i in rated :\n    print(i)\n    \nprint(\"Recommendations ... :\")\ncompare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4cdb734a2e827b128fcbb64f63644fb9fc672ed"},"cell_type":"code","source":"rated, compare = comparison(8, 5)\n\nprint(\"User has seen ... :\")\nfor i in rated :\n    print(i)\n    \nprint(\"Recommendations ... :\")\ncompare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0970ea672bde5938f98dc55bf2df55a97771126a"},"cell_type":"code","source":"rated, compare = comparison(55, 5)\n\nprint(\"User has seen ... :\")\nfor i in rated :\n    print(i)\n    \nprint(\"Recommendations ... :\")\ncompare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f21bf4e11b9364ebcb9842033f9df34e5b485bb"},"cell_type":"code","source":"rated, compare = comparison(5, 5)\n\nprint(\"User has seen ... :\")\nfor i in rated :\n    print(i)\n    \nprint(\"Recommendations ... :\")\ncompare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10db192ce5a1092342f4a8da6b6def3ec4135b4"},"cell_type":"code","source":"rated, compare = comparison(28, 5)\n\nprint(\"User has seen ... :\")\nfor i in rated :\n    print(i)\n    \nprint(\"Recommendations ... :\")\ncompare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0ddffbec78e23eebef99a60bd6e0f565f9e8362"},"cell_type":"markdown","source":"# Graph Based"},{"metadata":{"trusted":true,"_uuid":"f1e587122c5211f99c8bd8b8333a8d6474213918","collapsed":true},"cell_type":"code","source":"import networkx as nx\n\nG =  nx.Graph() #create the empty graph\n#users nodes\nG.add_nodes_from(interactions_full_df[\"user_id\"], bipartite=\"Users\")\n#animes nodes\nG.add_nodes_from(interactions_full_df[\"anime_id\"], bipartite=\"Animes\")\n#edge between users and animes\nG.add_edges_from(zip(interactions_full_df[\"anime_id\"], interactions_full_df[\"user_id\"]))\n\n\"\"\" bipartites projection \"\"\"\nusers_nodes =  [n for n in G.nodes() if G.node[n]\n                [\"bipartite\"] ==  \"Users\"]\nanimes_nodes = [n for n in G.nodes() if G.node[n]\n                [\"bipartite\"] ==  \"Animes\"]\nUsersG = nx.bipartite.projected_graph(G, nodes = users_nodes)\nAnimeG =  nx.bipartite.projected_graph(G, nodes = animes_nodes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7664cdd4a1286c51bd336962b0f5f5a55c410a","collapsed":true},"cell_type":"code","source":"def shared_partition_nodes(G, node1, node2):\n    # Check that the nodes belong to the same partition\n    #assert G.node[node1]['bipartite'] == G.node[node2]['bipartite']\n\n    # Get neighbors of node 1: nbrs1\n    nbrs1 = G.neighbors(node1)\n    # Get neighbors of node 2: nbrs2\n    nbrs2 = G.neighbors(node2)\n\n    # Compute the overlap using set intersections\n    overlap = set(nbrs1).intersection(nbrs2)\n    return overlap\n\n# Print the number of shared repositories between users 'u7909' and 'u2148'\nprint(len(shared_partition_nodes(G, 38, 20)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"413bda72901826276f2239ddb45bd41d4a570558"},"cell_type":"code","source":"# Define get_nodes_from_partition()\ndef get_nodes_from_partition(G, partition):\n    # Initialize an empty list for nodes to be returned\n    nodes = []\n    # Iterate over each node in the graph G\n    for n in G.nodes():\n        # Check that the node belongs to the particular partition\n        if G.node[n]['bipartite'] == partition:\n            # If so, append it to the list of nodes\n            nodes.append(n)\n    return nodes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d234bdbbdcee1668c99870c869c7a7de33d094f9","collapsed":true},"cell_type":"code","source":"def anime_similarity(G, anime1, anime2, users_proj_nodes) : \n    shared_nodes = shared_partition_nodes(G, anime1, anime2)\n    return len(shared_nodes) / len(users_proj_nodes)\n\n\ndef user_similarity(G, user1, user2, proj_nodes):\n    shared_nodes = shared_partition_nodes(G, user1, user2)\n    return len(shared_nodes) / len(proj_nodes)\n\n# Compute the similarity score between users 'u4560' and 'u1880'\nanimes_nodes = get_nodes_from_partition(G, 'Animes')\nusers_nodes = get_nodes_from_partition(G, 'Users')\nsimilarity_score = user_similarity(G, 38, 20, animes_nodes)\nsimilarity_score_anime = anime_similarity(G, 1, 6, users_nodes)\nprint(similarity_score)\nprint(\"similarity between trigun and cowboy bebop is {0}\".format(similarity_score_anime))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"222628bcb1a1bfcd981ddf8044b87e88fcada6b3","collapsed":true},"cell_type":"code","source":"anime_name =anime[[\"name\", \"anime_id\"]]\ntop_n_dict = {}\ndef most_similar_to(anime_id) : \n    #compute similarity dict\n    for others_animes in interactions_full_df[\"anime_id\"].unique() : \n        top_n_dict.update({others_animes : anime_similarity(G, anime_id, others_animes, users_nodes)})\n        \n    sorted_dict = [(key, value) for (key, value) in sorted(top_n_dict.items(), reverse = False)][1:15]\n    dtf = pd.DataFrame(sorted_dict).rename(columns={0:\"anime_id\", 1:\"most_similar_score\"})\n    merged_topn = dtf.merge(anime_name, left_on = \"anime_id\", right_on = \"anime_id\")\\\n    .sort_values(by=\"most_similar_score\", ascending = False)\n    return merged_topn\n    \nmerged_topn = most_similar_to(38)\nmerged_topn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76423261c0bf15849902c7e43914668fda43caa6","collapsed":true},"cell_type":"code","source":"from collections import defaultdict\n\ndef most_similar_users(G, user, user_nodes, proj_nodes):\n    # Data checks\n    assert G.node[user]['bipartite'] == 'Users'\n\n    # Get other nodes from user partition\n    user_nodes = set(user_nodes) \n    user_nodes.remove(user)\n\n    # Create the dictionary: similarities\n    similarities = defaultdict(list)\n    for n in user_nodes:\n        similarity = user_similarity(G, user, n, proj_nodes)\n        similarities[similarity].append(n)\n\n    # Compute maximum similarity score: max_similarity\n    max_similarity = max(similarities.keys())\n\n    # Return list of users that share maximal similarity\n    return similarities[max_similarity]\n\nuser_nodes = get_nodes_from_partition(G, 'Users')\nanimes_nodes = get_nodes_from_partition(G, 'Animes')\n\nprint(most_similar_users(G, 38, user_nodes, animes_nodes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4582775ba095c2b57f03843936718f89fe273d8a","collapsed":true},"cell_type":"code","source":"class NetworkRecommender : \n        \n    \"\"\" steps of a functional recommender :\n    step 1 : return 10 users most similar to our \n    step 2 : look every anime watched by similar users \n    step 3 : substract anime not seen by our users in others users list \n    step 4 : compute most similar anime between our users watched list and others users watched list\n    step 5 : return top_n\"\"\"\n    \n    MODEL_NAME = 'Network Recommender'\n    \n    def __init__(self, graph, animes_nodes, users_nodes) :\n        self.graph = graph\n        self.animes_nodes = animes_nodes\n        self.users_nodes = users_nodes\n    \n    def get_model_name(self):\n        return self.MODEL_NAME\n    \n    def anime_similarity(self, graph, anime1, anime2, users_nodes) : \n        shared_nodes = shared_partition_nodes(self.graph, anime1, anime2)\n        return len(shared_nodes) / len(users_nodes)\n\n    def user_similarity(self, graph, user1, user2, animes_nodes):\n        shared_nodes = shared_partition_nodes(self.graph, user1, user2)\n        return len(shared_nodes) / len(animes_nodes)\n    \n    def most_similar_users(self, sgraph, user_id, user_nodes, sanimes_nodes):\n        # Get other nodes from user partition\n        user_nodes = set(self.user_nodes) \n        user_nodes.remove(user_id)\n        # Create the dictionary: similarities\n        similarities = defaultdict(list)\n        for n in user_nodes:\n            similarity = user_similarity(self.graph, user_id, n, self.animes_nodes)\n            similarities[similarity].append(n)\n        # Compute maximum similarity score: max_similarity\n        max_similarity = max(similarities.keys())\n        # Return list of users that share maximal similarity\n        return similarities[max_similarity]\n    \n    def most_similar(self, anime_id) : \n        for others_animes in interactions_full_df[\"anime_id\"].unique() : \n            top_n_dict.update({others_animes : self.anime_similarity(self.graph, anime_id, \n                                                                 others_animes, self.users_nodes)})\n        #sort_it and reconstruct in a dataframe\n        sorted_dict = [(key, value) for (key, value) in sorted(top_n_dict.items(), reverse = False)][1:15]\n        dtf = pd.DataFrame(sorted_dict).rename(columns={0:\"anime_id\", 1:\"most_similar_score\"})\n        merged_topn = dtf.merge(anime_name, left_on = \"anime_id\", right_on = \"anime_id\")\\\n        .sort_values(by=\"most_similar_score\", ascending = False)\n        return merged_topn\n    \n    def recommend_anime(self, user_id, anime_to_ignore=[], topn=10, verbose=False):\n        merged_topn = self.most_similar(anime_id)\n        # Recommend the more popular items that the user hasn't seen yet.\n        recommendations_df = self.popularity_df[~self.popularity_df['anime_id'].isin(anime_to_ignore)] \\\n                               .sort_values('user_rating', ascending = False) \\\n                               .head(topn)\n    \nNxRec= NetworkRecommender(G, animes_nodes, users_nodes)\nNxRec.recommend_anime(38)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c4257c9d54864a854b3cd403ae88ddb8ba2f940","collapsed":true},"cell_type":"code","source":"def recommend_repositories(G, from_user, to_user):\n    # Get the set of repositories that from_user has contributed to\n    from_repos = set(G.neighbors(from_user))\n    # Get the set of repositories that to_user has contributed to\n    to_repos = set(G.neighbors(to_user))\n\n    # Identify repositories that the from_user is connected to that the to_user is not connected to\n    return from_repos.difference(to_repos)\n\n# Print the repositories to be recommended\nprint(recommend_repositories(G, 38, 20))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}