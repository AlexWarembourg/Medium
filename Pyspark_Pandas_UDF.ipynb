{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.55:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "# global setup to work around with pandas udf\n",
    "# ! sudo pip3 install pyarrow=0.14.1\n",
    "# see answers here https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception\n",
    "os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\n",
    "\n",
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define an output schema\n",
    "schema = StructType([\n",
    "        StructField(\"store\", StringType(), True),\n",
    "        StructField(\"item\", StringType(), True),\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetHolidays():\n",
    "    playoffs = pd.DataFrame({\n",
    "        'holiday': 'playoff',\n",
    "        'ds': pd.to_datetime(['2013-01-12', '2013-07-12', '2013-12-24',\n",
    "                              '2014-01-12', '2014-07-12', '2014-07-19',\n",
    "                              '2014-07-02', '2014-12-24', '2015-07-11', '2015-12-24',\n",
    "                              '2016-07-17', '2016-07-24', '2016-07-07',\n",
    "                              '2016-07-24', '2016-12-24', '2017-07-17', '2017-07-24',\n",
    "                              '2017-07-07', '2017-12-24']),\n",
    "        'lower_window': 0,\n",
    "        'upper_window': 2}\n",
    "    )\n",
    "    superbowls = pd.DataFrame({\n",
    "        'holiday': 'superbowl',\n",
    "        'ds': pd.to_datetime(['2013-01-01', '2013-01-21', '2013-02-14', '2013-02-18',\n",
    "                              '2013-05-27', '2013-07-04', '2013-09-02', '2013-10-14', '2013-11-11', '2013-11-28',\n",
    "                              '2013-12-25', '2014-01-01', '2014-01-20', '2014-02-14', '2014-02-17',\n",
    "                              '2014-05-26', '2014-07-04', '2014-09-01', '2014-10-13', '2014-11-11', '2014-11-27',\n",
    "                              '2014-12-25', '2015-01-01', '2015-01-19', '2015-02-14', '2015-02-16',\n",
    "                              '2015-05-25', '2015-07-03', '2015-09-07', '2015-10-12', '2015-11-11', '2015-11-26',\n",
    "                              '2015-12-25', '2016-01-01', '2016-01-18', '2016-02-14', '2016-02-15',\n",
    "                              '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24',\n",
    "                              '2016-12-25', '2017-01-02', '2017-01-16', '2017-02-14', '2017-02-20',\n",
    "                              '2017-05-29', '2017-07-04', '2017-09-04', '2017-10-09', '2017-11-10', '2017-11-23',\n",
    "                              '2017-12-25', '2018-01-01', '2018-01-15', '2018-02-14', '2018-02-19'\n",
    "                              ]),\n",
    "        'lower_window': 0,\n",
    "        'upper_window': 3,\n",
    "    })\n",
    "\n",
    "    holidays = pd.concat((playoffs, superbowls))\n",
    "    return holidays\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    rows have to be identified as train or test data using a col called 'train' as a boolean\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        # train\n",
    "        ts_train = (df\n",
    "                    .query('date <= @cutoff')\n",
    "                    .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )\n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('date > @cutoff')\n",
    "                   .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )\n",
    "\n",
    "        print(ts_test.columns)\n",
    "        # get holidays\n",
    "        holidays = GetHolidays()\n",
    "        # init model\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True,\n",
    "                    holidays=holidays)\n",
    "        m.fit(ts_train)\n",
    "\n",
    "        # to date\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  # merge to retrieve item and store index\n",
    "        # debug\n",
    "        # print(ts_hat)\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .config('spark.sql.execution.arrow.enable', 'true')\n",
    "             .getOrCreate()\n",
    "             )\n",
    "\n",
    "    # read input data from :https://www.kaggle.com/c/demand-forecasting-kernels-only/data\n",
    "    data_train = (spark\n",
    "                  .read\n",
    "                  .format(\"csv\")\n",
    "                  .option('header', 'true')\n",
    "                  .load('Downloads/train.csv')\n",
    "                  )\n",
    "\n",
    "    data_test = (spark\n",
    "                 .read\n",
    "                 .format(\"csv\")\n",
    "                 .option('header', 'true')\n",
    "                 .load('Downloads/test.csv')\n",
    "                 .drop('id')\n",
    "                 )\n",
    "    # max train date\n",
    "    cutoff = data_train.select(max(col('date'))).collect()[0][0]\n",
    "    # add sales none col to match with union\n",
    "    data_test = data_test.withColumn('sales', lit(None))\n",
    "    # concat train test\n",
    "    df = (data_train.union(data_test)).sort(col('date'))\n",
    "    # fit\n",
    "    global_predictions = (df\n",
    "                          .groupBy(\"store\", \"item\")\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------------+\n",
      "|store|item|        ds|              yhat|\n",
      "+-----+----+----------+------------------+\n",
      "|    1|  41|2018-01-01|15.472295798271924|\n",
      "|    1|  41|2018-01-02|17.020335423754645|\n",
      "|    1|  41|2018-01-03|16.214265572228584|\n",
      "|    1|  41|2018-01-04| 17.23924859630219|\n",
      "|    1|  41|2018-01-05| 19.08942632045219|\n",
      "|    1|  41|2018-01-06|20.668644854326548|\n",
      "|    1|  41|2018-01-07| 20.98480600016774|\n",
      "|    1|  41|2018-01-08|12.761782050104117|\n",
      "|    1|  41|2018-01-09|15.386374982415058|\n",
      "|    1|  41|2018-01-10|15.506009210853609|\n",
      "|    1|  41|2018-01-11| 16.33046994736669|\n",
      "|    1|  41|2018-01-12|17.970365581705174|\n",
      "|    1|  41|2018-01-13|19.606565165268467|\n",
      "|    1|  41|2018-01-14|20.005508549304146|\n",
      "|    1|  41|2018-01-15|13.533294786475976|\n",
      "|    1|  41|2018-01-16|15.152247680199745|\n",
      "|    1|  41|2018-01-17|14.462466577098105|\n",
      "|    1|  41|2018-01-18|15.645879613200684|\n",
      "|    1|  41|2018-01-19|17.692354852436257|\n",
      "|    1|  41|2018-01-20| 19.50056105771265|\n",
      "+-----+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python sample try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Downloads/train.csv')\n",
    "train[\"item\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train = pd.read_csv('Downloads/train.csv')\n",
    "\n",
    "    test = (pd.read_csv('Downloads/test.csv')\n",
    "            .assign(sales = lambda x : None)\n",
    "            .drop(\"id\", axis = 1)\n",
    "           )\n",
    "\n",
    "    cutoff = train[\"date\"].max()\n",
    "\n",
    "    df = (pd.concat((train, test))\n",
    "          .query('store == 1 and item == 1')\n",
    "         )\n",
    "\n",
    "    ts_train = (df\n",
    "                    .query('date <= @cutoff')\n",
    "                    .rename(columns={'date':'ds', 'sales':'y'})\n",
    "                    .sort_values('ds')\n",
    "                   )\n",
    "    # test\n",
    "    ts_test = (df\n",
    "               .query('date > @cutoff')\n",
    "               .rename(columns={'date':'ds', 'sales':'y'})\n",
    "               .sort_values('ds')\n",
    "          )\n",
    "    # get holidays \n",
    "    holidays = GetHolidays()\n",
    "    # init model\n",
    "    m =Prophet(yearly_seasonality=True, \n",
    "               weekly_seasonality=True,\n",
    "               daily_seasonality=True, \n",
    "               holidays=holidays)\n",
    "    m.fit(ts_train)\n",
    "    # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "    ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "              .assign(ds = lambda x : pd.to_datetime(x[\"ds\"]))\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
